{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import libraries and define common helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import gzip\n",
    "import json\n",
    "from pathlib import Path\n",
    "import csv\n",
    "\n",
    "import pandas as pd\n",
    "import s3fs\n",
    "import pyarrow as pa\n",
    "from pyarrow.json import read_json\n",
    "import pyarrow.parquet as pq\n",
    "import fastavro\n",
    "import pygeohash\n",
    "import snappy\n",
    "import jsonschema\n",
    "from jsonschema.exceptions import ValidationError\n",
    "\n",
    "\n",
    "endpoint_url='https://storage.budsc.midwest-datascience.com'\n",
    "\n",
    "current_dir = Path(os.getcwd()).absolute()\n",
    "schema_dir = current_dir.joinpath('schemas')\n",
    "results_dir = current_dir.joinpath('results')\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "def read_jsonl_data():\n",
    "    s3 = s3fs.S3FileSystem(\n",
    "        anon=True,\n",
    "        client_kwargs={\n",
    "            'endpoint_url': endpoint_url\n",
    "        }\n",
    "    )\n",
    "    src_data_path = 'data/processed/openflights/routes.jsonl.gz'\n",
    "    with s3.open(src_data_path, 'rb') as f_gz:\n",
    "        with gzip.open(f_gz, 'rb') as f:\n",
    "            records = [json.loads(line) for line in f.readlines()]\n",
    "        \n",
    "\n",
    "    return records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the records from https://storage.budsc.midwest-datascience.com/data/processed/openflights/routes.jsonl.gz "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = read_jsonl_data()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"airline\": {\n",
      "        \"airline_id\": 410,\n",
      "        \"name\": \"Aerocondor\",\n",
      "        \"alias\": \"ANA All Nippon Airways\",\n",
      "        \"iata\": \"2B\",\n",
      "        \"icao\": \"ARD\",\n",
      "        \"callsign\": \"AEROCONDOR\",\n",
      "        \"country\": \"Portugal\",\n",
      "        \"active\": true\n",
      "    },\n",
      "    \"src_airport\": {\n",
      "        \"airport_id\": 6156,\n",
      "        \"name\": \"Belgorod International Airport\",\n",
      "        \"city\": \"Belgorod\",\n",
      "        \"country\": \"Russia\",\n",
      "        \"iata\": \"EGO\",\n",
      "        \"icao\": \"UUOB\",\n",
      "        \"latitude\": 50.643798828125,\n",
      "        \"longitude\": 36.5900993347168,\n",
      "        \"altitude\": 735,\n",
      "        \"timezone\": 3.0,\n",
      "        \"dst\": \"N\",\n",
      "        \"tz_id\": \"Europe/Moscow\",\n",
      "        \"type\": \"airport\",\n",
      "        \"source\": \"OurAirports\"\n",
      "    },\n",
      "    \"dst_airport\": {\n",
      "        \"airport_id\": 2990,\n",
      "        \"name\": \"Kazan International Airport\",\n",
      "        \"city\": \"Kazan\",\n",
      "        \"country\": \"Russia\",\n",
      "        \"iata\": \"KZN\",\n",
      "        \"icao\": \"UWKD\",\n",
      "        \"latitude\": 55.606201171875,\n",
      "        \"longitude\": 49.278701782227,\n",
      "        \"altitude\": 411,\n",
      "        \"timezone\": 3.0,\n",
      "        \"dst\": \"N\",\n",
      "        \"tz_id\": \"Europe/Moscow\",\n",
      "        \"type\": \"airport\",\n",
      "        \"source\": \"OurAirports\"\n",
      "    },\n",
      "    \"codeshare\": false,\n",
      "    \"equipment\": [\n",
      "        \"CR2\"\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(records[10], indent =4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.a JSON Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def validate_jsonl_data(records):\n",
    "    schema_path = schema_dir.joinpath('routes-schema.json')\n",
    "    with open(schema_path) as f:\n",
    "        schema = json.load(f)\n",
    "    \n",
    "    validation_csv_path = results_dir.joinpath('json_schema_validation_results.csv')\n",
    "    with open(validation_csv_path, 'w') as f:\n",
    "        \n",
    "        columnnames = ['row_num', 'record', 'is_valid']\n",
    "        csv_writer = csv.DictWriter(f,fieldnames=columnnames, lineterminator ='\\n')\n",
    "        csv_writer.writeheader()\n",
    "        for i, record in enumerate(records):\n",
    "            try:\n",
    "                ## TODO: Validate record \n",
    "                #pass\n",
    "                jsonschema.validate(instance=record, schema = schema)\n",
    "                #result = dict(row_num = i, is_valid = True)\n",
    "            except ValidationError as e:\n",
    "                ## Print message if invalid record\n",
    "                #pass\n",
    "                \n",
    "                result = dict(row_num = i, record = record, is_valid = False)\n",
    "                csv_writer.writerow(result)\n",
    "                #pass\n",
    "            #finally:\n",
    "                \n",
    "            \n",
    "\n",
    "validate_jsonl_data(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    892\n",
       "Name: is_valid, dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"results/json_schema_validation_results.csv\")['is_valid'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.b Avro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_avro_dataset(records):\n",
    "    schema_path = schema_dir.joinpath('routes.avsc')\n",
    "    data_path = results_dir.joinpath('routes.avro')\n",
    "    ## TODO: Use fastavro to create Avro dataset\n",
    "    \n",
    "    with open(schema_path,'r') as f:\n",
    "        schema = json.load(f)\n",
    "    \n",
    "    parsed_schema = fastavro.parse_schema(schema)\n",
    "    \n",
    "    #, encoding=\"utf-8\"\n",
    "    with open(data_path, 'wb') as out:\n",
    "        fastavro.writer(out, parsed_schema , records)\n",
    "    \n",
    "        \n",
    "create_avro_dataset(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.c Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_parquet_dataset():\n",
    "    src_data_path = 'data/processed/openflights/routes.jsonl.gz'\n",
    "    parquet_output_path = results_dir.joinpath('routes.parquet')\n",
    "    s3 = s3fs.S3FileSystem(\n",
    "        anon=True,\n",
    "        client_kwargs={\n",
    "            'endpoint_url': endpoint_url\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    with s3.open(src_data_path, 'rb') as f_gz:\n",
    "        with gzip.open(f_gz, 'rb') as f:\n",
    "            #pass\n",
    "            ## TODO: Use Apache Arrow to create Parquet table and save the dataset\n",
    "            \n",
    "            parquet_table = read_json(f)\n",
    "            #print(parquet_table)\n",
    "            pq.write_table(parquet_table , parquet_output_path)\n",
    "            \n",
    "\n",
    "create_parquet_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.d Protocol Buffers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, os.path.abspath('routes_pb2'))\n",
    "\n",
    "import routes_pb2\n",
    "import google.protobuf.json_format as pbjf\n",
    "#from proto import apiai_pb2\n",
    "\n",
    "def _airport_to_proto_obj(airport):\n",
    "    obj = routes_pb2.Airport()\n",
    "    if airport is None:\n",
    "        # Returning None is raising a TypeError, so returning empty object\n",
    "        return None\n",
    "#         return obj\n",
    "    if airport.get('airport_id') is None:\n",
    "        # Returning None is raising a TypeError, so returning empty object\n",
    "        return None\n",
    "#         return obj\n",
    "\n",
    "    obj.airport_id = airport.get('airport_id')\n",
    "    if airport.get('name'):\n",
    "        obj.name = airport.get('name')\n",
    "    if airport.get('city'):\n",
    "        obj.city = airport.get('city')\n",
    "    if airport.get('iata'):\n",
    "        obj.iata = airport.get('iata')\n",
    "    if airport.get('icao'):\n",
    "        obj.icao = airport.get('icao')\n",
    "    if airport.get('altitude'):\n",
    "        obj.altitude = airport.get('altitude')\n",
    "    if airport.get('timezone'):\n",
    "        obj.timezone = airport.get('timezone')\n",
    "    if airport.get('dst'):\n",
    "        obj.dst = airport.get('dst')\n",
    "    if airport.get('tz_id'):\n",
    "        obj.tz_id = airport.get('tz_id')\n",
    "    if airport.get('type'):\n",
    "        obj.type = airport.get('type')\n",
    "    if airport.get('source'):\n",
    "        obj.source = airport.get('source')\n",
    "\n",
    "    obj.latitude = airport.get('latitude')\n",
    "    obj.longitude = airport.get('longitude')\n",
    "\n",
    "    return obj\n",
    "\n",
    "\n",
    "def _airline_to_proto_obj(airline):\n",
    "    obj = routes_pb2.Airline()\n",
    "    if not airline.get('name') is None:\n",
    "        return None\n",
    "    if not airline.get('airline_id') is None:\n",
    "        return None\n",
    "    obj.airline_id = airline.get('airline_id')\n",
    "    obj.name = airline.get('name')\n",
    "    if airline.get('alias'):\n",
    "        obj.alias = airline.get('alias')\n",
    "    ## TODO: Create an Airline obj using Protocol Buffers API\n",
    "    \"\"\"\n",
    "    Added by Tinto\n",
    "    \"\"\"\n",
    "    if airline.get('iata'):\n",
    "        obj.iata = airline.get('iata')\n",
    "    if airline.get('icao'):\n",
    "        obj.icao = airline.get('icao')\n",
    "    if airline.get('callsign'):\n",
    "        obj.callsign = airline.get('callsign')\n",
    "    if airline.get('country'):\n",
    "        obj.country = airline.get('country')\n",
    "    if airline.get('active'):\n",
    "        obj.active = airline.get('active')\n",
    "    \"\"\"\n",
    "    Added by Tinto end\n",
    "    \"\"\"        \n",
    "    return obj\n",
    "\n",
    "\n",
    "def create_protobuf_dataset(records):\n",
    "    routes = routes_pb2.Routes()\n",
    "    for record in records:\n",
    "        route = routes_pb2.Route()\n",
    "        \n",
    "        airline = _airline_to_proto_obj(record.get('airline', {}))\n",
    "        if airline:\n",
    "            route.airline.CopyFrom(airline)\n",
    "        src_airport = _airport_to_proto_obj(record.get('src_airport', {}))\n",
    "        ## TODO\n",
    "        if src_airport:\n",
    "            route.src_airport.CopyFrom(src_airport)\n",
    "        \n",
    "        dst_airport = _airport_to_proto_obj(record.get('dst_airport', {}))\n",
    "        if dst_airport:\n",
    "            route.dst_airport.CopyFrom(dst_airport)\n",
    "        \n",
    "        route.codeshare = record['codeshare']\n",
    "#         route.equipment[:] = record['equipment']\n",
    "\n",
    "        stops = _airport_to_proto_obj(record.get('stops', {}))\n",
    "        if stops:\n",
    "            route.dst_airport.CopyFrom(stops)\n",
    "            \n",
    "        equipment = record.get('equipment')\n",
    "        \n",
    "        routes.route.append(route)\n",
    "\n",
    "    data_path = results_dir.joinpath('routes.pb')\n",
    "\n",
    "    with open(data_path, 'wb') as f:\n",
    "        f.write(routes.SerializeToString())\n",
    "        \n",
    "    compressed_path = results_dir.joinpath('routes.pb.snappy')\n",
    "    \n",
    "    with open(compressed_path, 'wb') as f:\n",
    "        f.write(snappy.compress(routes.SerializeToString()))\n",
    "        \n",
    "create_protobuf_dataset(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.a Simple Geohash Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hash_dirs(records):\n",
    "    geoindex_dir = results_dir.joinpath('geoindex')\n",
    "    geoindex_dir.mkdir(exist_ok=True, parents=True)\n",
    "    hashes = []\n",
    "    ## TODO: Create hash index\n",
    "    for record in records:\n",
    "        src_airport = record.get('src_airport', {})\n",
    "        if src_airport:\n",
    "            latitude = src_airport.get('latitude')\n",
    "            longitude = src_airport.get('longitude')\n",
    "            if latitude and longitude:\n",
    "#                 hashes.append(pygeohash.encode(latitude, longitude, precision = 3))\n",
    "                geohash = pygeohash.encode(latitude, longitude)\n",
    "                record[\"geohash\"] = geohash\n",
    "#                 geohash = record.set('geohash')\n",
    "                hashes.append(geohash)\n",
    "#                 \n",
    "    hashes.sort()\n",
    "#     print(hashes)\n",
    "    three_letter = sorted(list(set([entry[:3] for entry in hashes])))\n",
    "#     print(three_letter)\n",
    "    hash_index = {value: [] for value in three_letter}\n",
    "    for record in records:\n",
    "        geohash = record.get('geohash')\n",
    "        if geohash:\n",
    "            hash_index[geohash[:3]].append(record)\n",
    "#     print(hash_index)\n",
    "    for key, values in hash_index.items():\n",
    "        output_dir = geoindex_dir.joinpath(str(key[:1])).joinpath(str(key[:2]))\n",
    "        output_dir.mkdir(exist_ok=True, parents=True)\n",
    "        output_path = output_dir.joinpath('{}.jsonl.gz'.format(key))\n",
    "        with gzip.open(output_path, 'w') as f:\n",
    "            json_output = '\\n'.join([json.dumps(value) for value in values])\n",
    "            f.write(json_output.encode('utf-8'))\n",
    "               \n",
    "create_hash_dirs(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Test to see if the data is getting generated with the files in the index\n",
    "\"\"\"\n",
    "\n",
    "def read_idx_data(src_data_path1):\n",
    "#     src_data_path1 = 'results/geoindex/9/9z/9z7.jsonl.gz'\n",
    "#     with open(src_data_path, 'rb') as f_gz:\n",
    "    with gzip.open(src_data_path1, 'rb') as f:\n",
    "        idx_records = [json.loads(line) for line in f.readlines()]\n",
    "        \n",
    "\n",
    "    return idx_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"airline\": {\"airline_id\": 24, \"name\": \"American Airlines\", \"alias\": \"\\\\N\", \"iata\": \"AA\", \"icao\": \"AAL\", \"callsign\": \"AMERICAN\", \"country\": \"United States\", \"active\": true}, \"src_airport\": {\"airport_id\": 3454, \"name\": \"Eppley Airfield\", \"city\": \"Omaha\", \"country\": \"United States\", \"iata\": \"OMA\", \"icao\": \"KOMA\", \"latitude\": 41.3032, \"longitude\": -95.89409599999999, \"altitude\": 984, \"timezone\": -6.0, \"dst\": \"A\", \"tz_id\": \"America/Chicago\", \"type\": \"airport\", \"source\": \"OurAirports\"}, \"dst_airport\": {\"airport_id\": 3876, \"name\": \"Charlotte Douglas International Airport\", \"city\": \"Charlotte\", \"country\": \"United States\", \"iata\": \"CLT\", \"icao\": \"KCLT\", \"latitude\": 35.2140007019043, \"longitude\": -80.94309997558594, \"altitude\": 748, \"timezone\": -5.0, \"dst\": \"A\", \"tz_id\": \"America/New_York\", \"type\": \"airport\", \"source\": \"OurAirports\"}, \"codeshare\": true, \"equipment\": [\"CR9\"], \"geohash\": \"9z7fczh09de3\"}\n"
     ]
    }
   ],
   "source": [
    "idx_records = read_idx_data('results/geoindex/9/9z/9z7.jsonl.gz')\n",
    "print(json.dumps(idx_records[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.b Simple Search Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3454\n",
      "Eppley Airfield\n"
     ]
    }
   ],
   "source": [
    "def airport_search(latitude, longitude):\n",
    "    ## TODO: Create simple search to return nearest airport\n",
    "#     pass\n",
    "    geohash1 = pygeohash.encode(latitude, longitude)\n",
    "#     print(geohash)\n",
    "    geoindex_dir = results_dir.joinpath('geoindex')\n",
    "    search_dir = geoindex_dir.joinpath(str(geohash1[:1])).joinpath(str(geohash1[:2]))\n",
    "#     print(search_dir)\n",
    "    search_file = search_dir.joinpath('{}.jsonl.gz'.format(geohash1[:3]))\n",
    "#     print(search_file)\n",
    "\n",
    "    idx_records = read_idx_data(search_file)\n",
    "#     print(json.dumps(idx_records[0]))\n",
    "    for record in idx_records:\n",
    "        src_airport = record.get('src_airport', {})\n",
    "        geohash = record.get('geohash')\n",
    "        \n",
    "#         print(json.dumps(src_airport))\n",
    "        if src_airport:\n",
    "            airport_id = src_airport.get('airport_id')\n",
    "            airport_name = src_airport.get('name')\n",
    "#             print(airport_name)\n",
    "            if geohash:\n",
    "#                 print(geohash)\n",
    "                if (pygeohash.geohash_approximate_distance(geohash, geohash1)/1000) < 100:\n",
    "                    airport_id = airport_id\n",
    "                    airport_name = airport_name\n",
    "                    \n",
    "    print(airport_id)\n",
    "    print(airport_name)    \n",
    "\n",
    "    \n",
    "airport_search(41.1499988, -95.91779)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
